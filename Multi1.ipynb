{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95ff8687-b732-43b4-9cb7-7c2d83878358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1444, 4)\n",
      "Category counts:\n",
      " category\n",
      "Group_B    709\n",
      "Group_C    481\n",
      "Group_A    254\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Original distribution: Counter({np.int64(1): 567, np.int64(2): 385, np.int64(0): 203})\n",
      "Resampled distribution: Counter({np.int64(1): 567, np.int64(2): 567, np.int64(0): 567})\n",
      "\n",
      "Training SVM with multi-kernel search...\n",
      "Best SVM params: {'C': 20, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "\n",
      "SVM Accuracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Group_A       1.00      1.00      1.00        51\n",
      "     Group_B       1.00      1.00      1.00       142\n",
      "     Group_C       1.00      1.00      1.00        96\n",
      "\n",
      "    accuracy                           1.00       289\n",
      "   macro avg       1.00      1.00      1.00       289\n",
      "weighted avg       1.00      1.00      1.00       289\n",
      "\n",
      "\n",
      "Training MLP...\n",
      "\n",
      "MLP Accuracy: 0.9965397923875432\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Group_A       0.98      1.00      0.99        51\n",
      "     Group_B       1.00      1.00      1.00       142\n",
      "     Group_C       1.00      0.99      0.99        96\n",
      "\n",
      "    accuracy                           1.00       289\n",
      "   macro avg       0.99      1.00      1.00       289\n",
      "weighted avg       1.00      1.00      1.00       289\n",
      "\n",
      "\n",
      "Training Random Forest...\n",
      "\n",
      "RF Accuracy: 0.9930795847750865\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Group_A       1.00      0.96      0.98        51\n",
      "     Group_B       0.99      1.00      1.00       142\n",
      "     Group_C       0.99      1.00      0.99        96\n",
      "\n",
      "    accuracy                           0.99       289\n",
      "   macro avg       0.99      0.99      0.99       289\n",
      "weighted avg       0.99      0.99      0.99       289\n",
      "\n",
      "\n",
      "Training Bagging SVM...\n",
      "\n",
      "Bagging SVM Accuracy: 0.9411764705882353\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Group_A       0.79      0.90      0.84        51\n",
      "     Group_B       1.00      0.97      0.99       142\n",
      "     Group_C       0.95      0.92      0.93        96\n",
      "\n",
      "    accuracy                           0.94       289\n",
      "   macro avg       0.91      0.93      0.92       289\n",
      "weighted avg       0.95      0.94      0.94       289\n",
      "\n",
      "\n",
      "Training Bagging MLP...\n",
      "\n",
      "Bagging MLP Accuracy: 0.9896193771626297\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Group_A       0.98      0.96      0.97        51\n",
      "     Group_B       0.99      1.00      1.00       142\n",
      "     Group_C       0.99      0.99      0.99        96\n",
      "\n",
      "    accuracy                           0.99       289\n",
      "   macro avg       0.99      0.98      0.99       289\n",
      "weighted avg       0.99      0.99      0.99       289\n",
      "\n",
      "\n",
      "Training AdaBoost...\n",
      "\n",
      "AdaBoost Accuracy: 0.7785467128027682\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Group_A       0.44      0.98      0.61        51\n",
      "     Group_B       1.00      0.56      0.71       142\n",
      "     Group_C       0.99      1.00      0.99        96\n",
      "\n",
      "    accuracy                           0.78       289\n",
      "   macro avg       0.81      0.85      0.77       289\n",
      "weighted avg       0.90      0.78      0.79       289\n",
      "\n",
      "\n",
      "Training Ensemble...\n",
      "\n",
      "ENSEMBLE Accuracy: 0.9965397923875432\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Group_A       0.98      1.00      0.99        51\n",
      "     Group_B       1.00      1.00      1.00       142\n",
      "     Group_C       1.00      0.99      0.99        96\n",
      "\n",
      "    accuracy                           1.00       289\n",
      "   macro avg       0.99      1.00      1.00       289\n",
      "weighted avg       1.00      1.00      1.00       289\n",
      "\n",
      "\n",
      "---------------- Accuracy Summary ----------------\n",
      "SVM: 1.0\n",
      "MLP: 0.9965397923875432\n",
      "RandomForest: 0.9930795847750865\n",
      "Bagging SVM: 0.9411764705882353\n",
      "Bagging MLP: 0.9896193771626297\n",
      "AdaBoost: 0.7785467128027682\n",
      "Ensemble: 0.9965397923875432\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, PolynomialFeatures\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "# ============================================================\n",
    "# Load data\n",
    "# ============================================================\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Category counts:\\n\", train['category'].value_counts())\n",
    "\n",
    "# ============================================================\n",
    "# Prepare features\n",
    "# ============================================================\n",
    "X = train[['signal_strength', 'response_level']]\n",
    "y = train['category']\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Polynomial features (degree 2)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_poly)\n",
    "\n",
    "# Train-validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_scaled, y_encoded, test_size=0.20, random_state=0, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# SMOTE oversampling\n",
    "print(\"\\nOriginal distribution:\", Counter(y_train))\n",
    "smote = SMOTE(random_state=0)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "print(\"Resampled distribution:\", Counter(y_train_res))\n",
    "\n",
    "# ============================================================\n",
    "# 1. SVM with multi-kernel GridSearch\n",
    "# ============================================================\n",
    "svm_params = [\n",
    "    {'kernel': ['linear'], 'C': [0.1, 1, 10, 20, 50]},\n",
    "    {'kernel': ['poly'], 'C': [1, 10, 20], 'degree': [2, 3, 4], 'gamma': ['scale', 0.01, 0.001]},\n",
    "    {'kernel': ['rbf'], 'C': [1, 10, 20], 'gamma': ['scale', 0.01, 0.001]}\n",
    "]\n",
    "\n",
    "grid_svm = GridSearchCV(\n",
    "    SVC(probability=True, random_state=0),\n",
    "    svm_params,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining SVM with multi-kernel search...\")\n",
    "grid_svm.fit(X_train_res, y_train_res)\n",
    "svm = grid_svm.best_estimator_\n",
    "\n",
    "y_pred_svm = svm.predict(X_val)\n",
    "acc_svm = accuracy_score(y_val, y_pred_svm)\n",
    "\n",
    "print(\"Best SVM params:\", grid_svm.best_params_)\n",
    "print(\"\\nSVM Accuracy:\", acc_svm)\n",
    "print(classification_report(y_val, y_pred_svm, target_names=le.classes_))\n",
    "\n",
    "# ============================================================\n",
    "# 2. Improved MLP\n",
    "# ============================================================\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(64, 32),\n",
    "    activation='tanh',\n",
    "    solver='adam',\n",
    "    alpha=0.001,\n",
    "    learning_rate_init=0.0005,\n",
    "    max_iter=700,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.15,\n",
    "    n_iter_no_change=20,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "print(\"\\nTraining MLP...\")\n",
    "mlp.fit(X_train_res, y_train_res)\n",
    "y_pred_mlp = mlp.predict(X_val)\n",
    "acc_mlp = accuracy_score(y_val, y_pred_mlp)\n",
    "\n",
    "print(\"\\nMLP Accuracy:\", acc_mlp)\n",
    "print(classification_report(y_val, y_pred_mlp, target_names=le.classes_))\n",
    "\n",
    "# ============================================================\n",
    "# 3. Random Forest\n",
    "# ============================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_depth=12,\n",
    "    min_samples_split=4,\n",
    "    min_samples_leaf=2,\n",
    "    class_weight='balanced_subsample',\n",
    "    n_jobs=-1,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Random Forest...\")\n",
    "rf.fit(X_train_res, y_train_res)\n",
    "y_pred_rf = rf.predict(X_val)\n",
    "acc_rf = accuracy_score(y_val, y_pred_rf)\n",
    "\n",
    "print(\"\\nRF Accuracy:\", acc_rf)\n",
    "print(classification_report(y_val, y_pred_rf, target_names=le.classes_))\n",
    "\n",
    "# ============================================================\n",
    "# 4. Tuned Bagging SVM (max performance)\n",
    "# ============================================================\n",
    "bag_svm = BaggingClassifier(\n",
    "    estimator=SVC(\n",
    "        C=50,                  # increased C\n",
    "        gamma=0.0005,           # smaller gamma for smoother decision\n",
    "        kernel='rbf',           # best kernel from tuning\n",
    "        probability=True,\n",
    "        random_state=0\n",
    "    ),\n",
    "    n_estimators=30,           # more bags\n",
    "    max_samples=0.9,           # use more data per bag\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Bagging SVM...\")\n",
    "bag_svm.fit(X_train_res, y_train_res)\n",
    "y_pred_bag_svm = bag_svm.predict(X_val)\n",
    "acc_bag_svm = accuracy_score(y_val, y_pred_bag_svm)\n",
    "\n",
    "print(\"\\nBagging SVM Accuracy:\", acc_bag_svm)\n",
    "print(classification_report(y_val, y_pred_bag_svm, target_names=le.classes_))\n",
    "\n",
    "# ============================================================\n",
    "# 5. Bagging MLP\n",
    "# ============================================================\n",
    "bag_mlp = BaggingClassifier(\n",
    "    estimator=MLPClassifier(\n",
    "        hidden_layer_sizes=(64, 32),\n",
    "        activation='tanh',\n",
    "        solver='adam',\n",
    "        alpha=0.001,\n",
    "        learning_rate_init=0.0005,\n",
    "        max_iter=500,\n",
    "        random_state=0\n",
    "    ),\n",
    "    n_estimators=10,\n",
    "    max_samples=1.0,    # use all samples\n",
    "    bootstrap=False,    # no bootstrap\n",
    "    n_jobs=-1,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Bagging MLP...\")\n",
    "bag_mlp.fit(X_train_res, y_train_res)\n",
    "y_pred_bag_mlp = bag_mlp.predict(X_val)\n",
    "acc_bag_mlp = accuracy_score(y_val, y_pred_bag_mlp)\n",
    "\n",
    "print(\"\\nBagging MLP Accuracy:\", acc_bag_mlp)\n",
    "print(classification_report(y_val, y_pred_bag_mlp, target_names=le.classes_))\n",
    "\n",
    "# ============================================================\n",
    "# 6. AdaBoost\n",
    "# ============================================================\n",
    "boost = AdaBoostClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.5,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "print(\"\\nTraining AdaBoost...\")\n",
    "boost.fit(X_train_res, y_train_res)\n",
    "y_pred_boost = boost.predict(X_val)\n",
    "acc_boost = accuracy_score(y_val, y_pred_boost)\n",
    "\n",
    "print(\"\\nAdaBoost Accuracy:\", acc_boost)\n",
    "print(classification_report(y_val, y_pred_boost, target_names=le.classes_))\n",
    "\n",
    "# ============================================================\n",
    "# 7. Voting Ensemble\n",
    "# ============================================================\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('svm', svm),\n",
    "        ('mlp', mlp),\n",
    "        ('rf', rf),\n",
    "        ('bag_svm', bag_svm),\n",
    "        ('bag_mlp', bag_mlp),\n",
    "        ('boost', boost)\n",
    "    ],\n",
    "    voting='soft',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Ensemble...\")\n",
    "ensemble.fit(X_train_res, y_train_res)\n",
    "y_pred_ens = ensemble.predict(X_val)\n",
    "acc_ens = accuracy_score(y_val, y_pred_ens)\n",
    "\n",
    "print(\"\\nENSEMBLE Accuracy:\", acc_ens)\n",
    "print(classification_report(y_val, y_pred_ens, target_names=le.classes_))\n",
    "\n",
    "# ============================================================\n",
    "# Predict on Test\n",
    "# ============================================================\n",
    "X_test = poly.transform(test[['signal_strength', 'response_level']])\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "models = {\n",
    "    \"svm\": svm,\n",
    "    \"mlp\": mlp,\n",
    "    \"random_forest\": rf,\n",
    "    \"bagging_svm\": bag_svm,\n",
    "    \"bagging_mlp\": bag_mlp,\n",
    "    \"adaboost\": boost,\n",
    "    \"ensemble\": ensemble\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    preds = model.predict(X_test)\n",
    "    preds = le.inverse_transform(preds)\n",
    "    pd.DataFrame({\n",
    "        \"sample_id\": test[\"sample_id\"],\n",
    "        \"category\": preds\n",
    "    }).to_csv(f\"{name}_predictions_02.csv\", index=False)\n",
    "\n",
    "# ============================================================\n",
    "# Accuracy Summary\n",
    "# ============================================================\n",
    "print(\"\\n---------------- Accuracy Summary ----------------\")\n",
    "print(\"SVM:\", acc_svm)\n",
    "print(\"MLP:\", acc_mlp)\n",
    "print(\"RandomForest:\", acc_rf)\n",
    "print(\"Bagging SVM:\", acc_bag_svm)\n",
    "print(\"Bagging MLP:\", acc_bag_mlp)\n",
    "print(\"AdaBoost:\", acc_boost)\n",
    "print(\"Ensemble:\", acc_ens)\n",
    "print(\"--------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040aace1-d22f-4354-8f76-25e935eba412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d4237ad-ed03-43f1-a37e-621823026c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1444, 4)\n",
      "Category counts:\n",
      " category\n",
      "Group_B    709\n",
      "Group_C    481\n",
      "Group_A    254\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Original distribution: Counter({np.int64(1): 141, np.int64(2): 96, np.int64(0): 51})\n",
      "Resampled distribution: Counter({np.int64(0): 141, np.int64(1): 141, np.int64(2): 141})\n",
      "\n",
      "Training SVM with multi-kernel search...\n",
      "Best SVM params: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "\n",
      "SVM Accuracy: 0.9852941176470589\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Group_A       0.93      1.00      0.96       203\n",
      "     Group_B       1.00      0.99      0.99       568\n",
      "     Group_C       1.00      0.97      0.99       385\n",
      "\n",
      "    accuracy                           0.99      1156\n",
      "   macro avg       0.97      0.99      0.98      1156\n",
      "weighted avg       0.99      0.99      0.99      1156\n",
      "\n",
      "\n",
      "Training MLP...\n",
      "\n",
      "MLP Accuracy: 0.9022491349480969\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Group_A       0.66      0.92      0.77       203\n",
      "     Group_B       0.99      0.92      0.95       568\n",
      "     Group_C       0.97      0.88      0.92       385\n",
      "\n",
      "    accuracy                           0.90      1156\n",
      "   macro avg       0.87      0.90      0.88      1156\n",
      "weighted avg       0.92      0.90      0.91      1156\n",
      "\n",
      "\n",
      "Training Random Forest...\n",
      "\n",
      "RF Accuracy: 0.9757785467128027\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Group_A       0.93      0.94      0.93       203\n",
      "     Group_B       0.99      0.98      0.99       568\n",
      "     Group_C       0.97      0.98      0.98       385\n",
      "\n",
      "    accuracy                           0.98      1156\n",
      "   macro avg       0.97      0.97      0.97      1156\n",
      "weighted avg       0.98      0.98      0.98      1156\n",
      "\n",
      "\n",
      "Training Bagging SVM...\n",
      "\n",
      "Bagging SVM Accuracy: 0.9247404844290658\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Group_A       0.72      0.93      0.81       203\n",
      "     Group_B       0.99      0.94      0.97       568\n",
      "     Group_C       0.97      0.90      0.93       385\n",
      "\n",
      "    accuracy                           0.92      1156\n",
      "   macro avg       0.90      0.92      0.90      1156\n",
      "weighted avg       0.94      0.92      0.93      1156\n",
      "\n",
      "\n",
      "Training Bagging MLP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohit/miniconda3/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/rohit/miniconda3/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/rohit/miniconda3/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/rohit/miniconda3/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/rohit/miniconda3/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/rohit/miniconda3/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/rohit/miniconda3/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/rohit/miniconda3/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bagging MLP Accuracy: 0.9749134948096886\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Group_A       0.89      0.98      0.93       203\n",
      "     Group_B       0.99      0.98      0.99       568\n",
      "     Group_C       1.00      0.97      0.98       385\n",
      "\n",
      "    accuracy                           0.97      1156\n",
      "   macro avg       0.96      0.97      0.97      1156\n",
      "weighted avg       0.98      0.97      0.98      1156\n",
      "\n",
      "\n",
      "Training AdaBoost...\n",
      "\n",
      "AdaBoost Accuracy: 0.8347750865051903\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Group_A       0.53      0.53      0.53       203\n",
      "     Group_B       0.86      0.94      0.90       568\n",
      "     Group_C       0.98      0.84      0.90       385\n",
      "\n",
      "    accuracy                           0.83      1156\n",
      "   macro avg       0.79      0.77      0.78      1156\n",
      "weighted avg       0.84      0.83      0.83      1156\n",
      "\n",
      "\n",
      "Training Ensemble...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohit/miniconda3/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/rohit/miniconda3/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/rohit/miniconda3/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/rohit/miniconda3/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/rohit/miniconda3/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/rohit/miniconda3/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/rohit/miniconda3/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/rohit/miniconda3/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ENSEMBLE Accuracy: 0.9775086505190311\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Group_A       0.90      0.99      0.94       203\n",
      "     Group_B       1.00      0.98      0.99       568\n",
      "     Group_C       0.99      0.97      0.98       385\n",
      "\n",
      "    accuracy                           0.98      1156\n",
      "   macro avg       0.96      0.98      0.97      1156\n",
      "weighted avg       0.98      0.98      0.98      1156\n",
      "\n",
      "\n",
      "---------------- Accuracy Summary ----------------\n",
      "SVM: 0.9852941176470589\n",
      "MLP: 0.9022491349480969\n",
      "RandomForest: 0.9757785467128027\n",
      "Bagging SVM: 0.9247404844290658\n",
      "Bagging MLP: 0.9749134948096886\n",
      "AdaBoost: 0.8347750865051903\n",
      "Ensemble: 0.9775086505190311\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, PolynomialFeatures\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "# ============================================================\n",
    "# Load data\n",
    "# ============================================================\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Category counts:\\n\", train['category'].value_counts())\n",
    "\n",
    "# ============================================================\n",
    "# Prepare features\n",
    "# ============================================================\n",
    "X = train[['signal_strength', 'response_level']]\n",
    "y = train['category']\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Polynomial features (degree 2)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_poly)\n",
    "\n",
    "# Train-validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_scaled, y_encoded, test_size=0.80, random_state=0, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# SMOTE oversampling\n",
    "print(\"\\nOriginal distribution:\", Counter(y_train))\n",
    "smote = SMOTE(random_state=0)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "print(\"Resampled distribution:\", Counter(y_train_res))\n",
    "\n",
    "# ============================================================\n",
    "# 1. SVM with multi-kernel GridSearch\n",
    "# ============================================================\n",
    "svm_params = [\n",
    "    {'kernel': ['linear'], 'C': [0.1, 1, 10, 20, 50]},\n",
    "    {'kernel': ['poly'], 'C': [1, 10, 20], 'degree': [2, 3, 4], 'gamma': ['scale', 0.01, 0.001]},\n",
    "    {'kernel': ['rbf'], 'C': [1, 10, 20], 'gamma': ['scale', 0.01, 0.001]}\n",
    "]\n",
    "\n",
    "grid_svm = GridSearchCV(\n",
    "    SVC(probability=True, random_state=0),\n",
    "    svm_params,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining SVM with multi-kernel search...\")\n",
    "grid_svm.fit(X_train_res, y_train_res)\n",
    "svm = grid_svm.best_estimator_\n",
    "\n",
    "y_pred_svm = svm.predict(X_val)\n",
    "acc_svm = accuracy_score(y_val, y_pred_svm)\n",
    "\n",
    "print(\"Best SVM params:\", grid_svm.best_params_)\n",
    "print(\"\\nSVM Accuracy:\", acc_svm)\n",
    "print(classification_report(y_val, y_pred_svm, target_names=le.classes_))\n",
    "\n",
    "# ============================================================\n",
    "# 2. Improved MLP\n",
    "# ============================================================\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(64, 32),\n",
    "    activation='tanh',\n",
    "    solver='adam',\n",
    "    alpha=0.001,\n",
    "    learning_rate_init=0.0005,\n",
    "    max_iter=700,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.15,\n",
    "    n_iter_no_change=20,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "print(\"\\nTraining MLP...\")\n",
    "mlp.fit(X_train_res, y_train_res)\n",
    "y_pred_mlp = mlp.predict(X_val)\n",
    "acc_mlp = accuracy_score(y_val, y_pred_mlp)\n",
    "\n",
    "print(\"\\nMLP Accuracy:\", acc_mlp)\n",
    "print(classification_report(y_val, y_pred_mlp, target_names=le.classes_))\n",
    "\n",
    "# ============================================================\n",
    "# 3. Random Forest\n",
    "# ============================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_depth=12,\n",
    "    min_samples_split=4,\n",
    "    min_samples_leaf=2,\n",
    "    class_weight='balanced_subsample',\n",
    "    n_jobs=-1,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Random Forest...\")\n",
    "rf.fit(X_train_res, y_train_res)\n",
    "y_pred_rf = rf.predict(X_val)\n",
    "acc_rf = accuracy_score(y_val, y_pred_rf)\n",
    "\n",
    "print(\"\\nRF Accuracy:\", acc_rf)\n",
    "print(classification_report(y_val, y_pred_rf, target_names=le.classes_))\n",
    "\n",
    "# ============================================================\n",
    "# 4. Tuned Bagging SVM (max performance)\n",
    "# ============================================================\n",
    "bag_svm = BaggingClassifier(\n",
    "    estimator=SVC(\n",
    "        C=50,                  # increased C\n",
    "        gamma=0.0005,           # smaller gamma for smoother decision\n",
    "        kernel='rbf',           # best kernel from tuning\n",
    "        probability=True,\n",
    "        random_state=0\n",
    "    ),\n",
    "    n_estimators=30,           # more bags\n",
    "    max_samples=0.9,           # use more data per bag\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Bagging SVM...\")\n",
    "bag_svm.fit(X_train_res, y_train_res)\n",
    "y_pred_bag_svm = bag_svm.predict(X_val)\n",
    "acc_bag_svm = accuracy_score(y_val, y_pred_bag_svm)\n",
    "\n",
    "print(\"\\nBagging SVM Accuracy:\", acc_bag_svm)\n",
    "print(classification_report(y_val, y_pred_bag_svm, target_names=le.classes_))\n",
    "\n",
    "# ============================================================\n",
    "# 5. Bagging MLP\n",
    "# ============================================================\n",
    "bag_mlp = BaggingClassifier(\n",
    "    estimator=MLPClassifier(\n",
    "        hidden_layer_sizes=(64, 32),\n",
    "        activation='tanh',\n",
    "        solver='adam',\n",
    "        alpha=0.001,\n",
    "        learning_rate_init=0.0005,\n",
    "        max_iter=500,\n",
    "        random_state=0\n",
    "    ),\n",
    "    n_estimators=10,\n",
    "    max_samples=1.0,    # use all samples\n",
    "    bootstrap=False,    # no bootstrap\n",
    "    n_jobs=-1,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Bagging MLP...\")\n",
    "bag_mlp.fit(X_train_res, y_train_res)\n",
    "y_pred_bag_mlp = bag_mlp.predict(X_val)\n",
    "acc_bag_mlp = accuracy_score(y_val, y_pred_bag_mlp)\n",
    "\n",
    "print(\"\\nBagging MLP Accuracy:\", acc_bag_mlp)\n",
    "print(classification_report(y_val, y_pred_bag_mlp, target_names=le.classes_))\n",
    "\n",
    "# ============================================================\n",
    "# 6. AdaBoost\n",
    "# ============================================================\n",
    "boost = AdaBoostClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.5,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "print(\"\\nTraining AdaBoost...\")\n",
    "boost.fit(X_train_res, y_train_res)\n",
    "y_pred_boost = boost.predict(X_val)\n",
    "acc_boost = accuracy_score(y_val, y_pred_boost)\n",
    "\n",
    "print(\"\\nAdaBoost Accuracy:\", acc_boost)\n",
    "print(classification_report(y_val, y_pred_boost, target_names=le.classes_))\n",
    "\n",
    "# ============================================================\n",
    "# 7. Voting Ensemble\n",
    "# ============================================================\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('svm', svm),\n",
    "        ('mlp', mlp),\n",
    "        ('rf', rf),\n",
    "        ('bag_svm', bag_svm),\n",
    "        ('bag_mlp', bag_mlp),\n",
    "        ('boost', boost)\n",
    "    ],\n",
    "    voting='soft',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Ensemble...\")\n",
    "ensemble.fit(X_train_res, y_train_res)\n",
    "y_pred_ens = ensemble.predict(X_val)\n",
    "acc_ens = accuracy_score(y_val, y_pred_ens)\n",
    "\n",
    "print(\"\\nENSEMBLE Accuracy:\", acc_ens)\n",
    "print(classification_report(y_val, y_pred_ens, target_names=le.classes_))\n",
    "\n",
    "# ============================================================\n",
    "# Predict on Test\n",
    "# ============================================================\n",
    "X_test = poly.transform(test[['signal_strength', 'response_level']])\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "models = {\n",
    "    \"svm\": svm,\n",
    "    \"mlp\": mlp,\n",
    "    \"random_forest\": rf,\n",
    "    \"bagging_svm\": bag_svm,\n",
    "    \"bagging_mlp\": bag_mlp,\n",
    "    \"adaboost\": boost,\n",
    "    \"ensemble\": ensemble\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    preds = model.predict(X_test)\n",
    "    preds = le.inverse_transform(preds)\n",
    "    pd.DataFrame({\n",
    "        \"sample_id\": test[\"sample_id\"],\n",
    "        \"category\": preds\n",
    "    }).to_csv(f\"{name}_predictions_08.csv\", index=False)\n",
    "\n",
    "# ============================================================\n",
    "# Accuracy Summary\n",
    "# ============================================================\n",
    "print(\"\\n---------------- Accuracy Summary ----------------\")\n",
    "print(\"SVM:\", acc_svm)\n",
    "print(\"MLP:\", acc_mlp)\n",
    "print(\"RandomForest:\", acc_rf)\n",
    "print(\"Bagging SVM:\", acc_bag_svm)\n",
    "print(\"Bagging MLP:\", acc_bag_mlp)\n",
    "print(\"AdaBoost:\", acc_boost)\n",
    "print(\"Ensemble:\", acc_ens)\n",
    "print(\"--------------------------------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
