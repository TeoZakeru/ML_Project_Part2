{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeed97d-77ce-4b45-856b-edb779ee4dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training quick baseline models for feature importance...\n",
      "\n",
      "Top 20 features by aggregated importance:\n",
      "feature\n",
      "sex_M                     1.922985\n",
      "hemoglobin_level          0.509274\n",
      "height_cm                 0.502163\n",
      "serum_creatinine          0.387072\n",
      "bp_systolic               0.330507\n",
      "weight_kg                 0.318126\n",
      "hearing_left              0.309947\n",
      "tartar_presence_Y         0.306049\n",
      "waist_circumference_cm    0.295470\n",
      "triglycerides             0.295104\n",
      "hearing_right             0.293218\n",
      "ggt_enzyme_level          0.291284\n",
      "hdl_cholesterol           0.287323\n",
      "age_group                 0.285846\n",
      "bp_diastolic              0.285536\n",
      "vision_right              0.267097\n",
      "vision_left               0.265859\n",
      "total_cholesterol         0.263803\n",
      "ldl_cholesterol           0.252161\n",
      "patient_id                0.251736\n",
      "Name: agg_mean, dtype: float64\n",
      "\n",
      "Selected top 10 features for feature engineering: ['sex_M', 'hemoglobin_level', 'height_cm', 'serum_creatinine', 'bp_systolic', 'weight_kg', 'hearing_left', 'tartar_presence_Y', 'waist_circumference_cm', 'triglycerides']\n",
      "\n",
      "Pruning 8 features with combined importance < 0.001\n",
      "Features remaining after prune: 59\n",
      "\n",
      "======== Test Size = 0.2 ========\n",
      "PCA reduced 59 → 17 features\n",
      "MLP Acc     = 0.7271\n",
      "LogReg Acc  = 0.7147\n",
      "Bayes Acc   = 0.7192\n",
      "SVM Acc     = 0.7469\n",
      "Best SVM Params: {'C': 3, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "\n",
      "======== Test Size = 0.8 ========\n",
      "PCA reduced 59 → 17 features\n",
      "MLP Acc     = 0.7261\n",
      "LogReg Acc  = 0.7151\n",
      "Bayes Acc   = 0.7204\n",
      "SVM Acc     = 0.7251\n",
      "Best SVM Params: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "\n",
      "All predictions saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# ---------------------------\n",
    "# CONFIG\n",
    "# ---------------------------\n",
    "TRAIN_PATH = \"train.csv\"\n",
    "TEST_PATH = \"test.csv\"\n",
    "TARGET = \"has_copd_risk\"\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# ---------------------------\n",
    "# LOAD\n",
    "# ---------------------------\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test = pd.read_csv(TEST_PATH)\n",
    "\n",
    "# ---------------------------\n",
    "# OUTLIER HANDLING (winsorize)\n",
    "# ---------------------------\n",
    "df_clean = train.copy()\n",
    "\n",
    "numeric_cols_raw = df_clean.select_dtypes(include=[\"float64\", \"int64\"]).columns.tolist()\n",
    "numeric_cols_raw = [c for c in numeric_cols_raw if c not in [\"patient_id\"]]\n",
    "\n",
    "for col in numeric_cols_raw:\n",
    "    # small limits to clip extreme outliers\n",
    "    df_clean[col] = winsorize(df_clean[col], limits=[0.01, 0.01])\n",
    "\n",
    "# ---------------------------\n",
    "# PREPROCESS: one-hot encode and target mapping\n",
    "# ---------------------------\n",
    "df = df_clean.copy()\n",
    "\n",
    "# identify categorical columns (object dtype)\n",
    "cat_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "cat_cols = [c for c in cat_cols if c != TARGET]\n",
    "\n",
    "df = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
    "\n",
    "# map target Y/N to 1/0 if necessary\n",
    "if df[TARGET].dtype == \"object\":\n",
    "    df[TARGET] = df[TARGET].map({\"Y\": 1, \"N\": 0})\n",
    "\n",
    "# prepare X,y (we will later expand X with engineered features)\n",
    "y = df[TARGET].astype(int)\n",
    "X = df.drop(columns=[TARGET])\n",
    "\n",
    "# fill missing numeric values (column means)\n",
    "X = X.fillna(X.mean())\n",
    "\n",
    "# ---------------------------\n",
    "# BASELINE IMPORTANCE COMPUTATION (no PCA)\n",
    "# ---------------------------\n",
    "# scale baseline X for model training\n",
    "scaler_for_importance = StandardScaler()\n",
    "X_scaled_for_imp = scaler_for_importance.fit_transform(X)\n",
    "\n",
    "# split for validation to compute permutation importance\n",
    "X_train0, X_val0, y_train0, y_val0 = train_test_split(\n",
    "    X_scaled_for_imp, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Training quick baseline models for feature importance...\")\n",
    "\n",
    "# quick baseline models (smaller training iterations to speed up)\n",
    "logreg0 = LogisticRegression(max_iter=500, class_weight=\"balanced\", random_state=RANDOM_STATE)\n",
    "logreg0.fit(X_train0, y_train0)\n",
    "\n",
    "bayes0 = GaussianNB()\n",
    "bayes0.fit(X_train0, y_train0)\n",
    "\n",
    "svm0 = SVC(kernel=\"rbf\", class_weight=\"balanced\", probability=False, random_state=RANDOM_STATE)\n",
    "svm0.fit(X_train0, y_train0)\n",
    "\n",
    "mlp0 = MLPClassifier(hidden_layer_sizes=(32, 16), max_iter=150, random_state=RANDOM_STATE)\n",
    "mlp0.fit(X_train0, y_train0)\n",
    "\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "# Helper importance functions\n",
    "def logreg_importance(model, feature_names):\n",
    "    coef = np.abs(model.coef_[0])\n",
    "    dfi = pd.DataFrame({\"feature\": feature_names, \"importance\": coef})\n",
    "    return dfi.sort_values(\"importance\", ascending=False).set_index(\"feature\")[\"importance\"]\n",
    "\n",
    "def bayes_importance(model, feature_names):\n",
    "    # GaussianNB sigma_ shape: (n_classes, n_features) -> use class 1 if binary\n",
    "    # Use inverse variance (1 / sigma) as simple importance proxy\n",
    "    # If sigma_ is zero (rare), add small eps\n",
    "    sigma = model.var_\n",
    "    if sigma.ndim == 2:\n",
    "        sigma_class = sigma[1] if sigma.shape[0] > 1 else sigma[0]\n",
    "    else:\n",
    "        sigma_class = sigma\n",
    "    importance = 1.0 / (sigma_class + 1e-8)\n",
    "    dfi = pd.DataFrame({\"feature\": feature_names, \"importance\": importance})\n",
    "    return dfi.sort_values(\"importance\", ascending=False).set_index(\"feature\")[\"importance\"]\n",
    "\n",
    "def perm_importance_wrapper(model, X_val, y_val, feature_names, n_repeats=5):\n",
    "    # returns series indexed by feature name\n",
    "    res = permutation_importance(model, X_val, y_val, scoring=\"accuracy\", n_repeats=n_repeats, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "    dfi = pd.DataFrame({\"feature\": feature_names, \"importance\": res.importances_mean})\n",
    "    return dfi.sort_values(\"importance\", ascending=False).set_index(\"feature\")[\"importance\"]\n",
    "\n",
    "# compute importances\n",
    "logreg_imp = logreg_importance(logreg0, feature_names)\n",
    "bayes_imp = bayes_importance(bayes0, feature_names)\n",
    "svm_imp = perm_importance_wrapper(svm0, X_val0, y_val0, feature_names, n_repeats=5)\n",
    "mlp_imp = perm_importance_wrapper(mlp0, X_val0, y_val0, feature_names, n_repeats=5)\n",
    "\n",
    "# align and aggregate (mean importance across models)\n",
    "imps_df = pd.concat([\n",
    "    logreg_imp.rename(\"logreg\"),\n",
    "    bayes_imp.rename(\"bayes\"),\n",
    "    svm_imp.rename(\"svm\"),\n",
    "    mlp_imp.rename(\"mlp\")\n",
    "], axis=1).fillna(0)\n",
    "\n",
    "imps_df[\"agg_mean\"] = imps_df.mean(axis=1)\n",
    "imps_df = imps_df.sort_values(\"agg_mean\", ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 features by aggregated importance:\")\n",
    "print(imps_df[\"agg_mean\"].head(20))\n",
    "\n",
    "# choose top features for engineering\n",
    "TOP_K = 10\n",
    "top_features = imps_df.index[:TOP_K].tolist()\n",
    "print(f\"\\nSelected top {TOP_K} features for feature engineering: {top_features}\")\n",
    "\n",
    "# ---------------------------\n",
    "# FEATURE ENGINEERING FUNCTIONS\n",
    "# ---------------------------\n",
    "def add_interactions_df(df_in, features, max_pairs=28):\n",
    "    \"\"\"Add pairwise multiplication interactions among provided features.\n",
    "       Limit to first max_pairs combinations to avoid explosion.\"\"\"\n",
    "    df = df_in.copy()\n",
    "    combos = []\n",
    "    n = len(features)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            combos.append((features[i], features[j]))\n",
    "    # limit combos deterministically\n",
    "    combos = combos[:max_pairs]\n",
    "    for f1, f2 in combos:\n",
    "        new_name = f\"{f1}_x_{f2}\"\n",
    "        df[new_name] = df[f1] * df[f2]\n",
    "    return df\n",
    "\n",
    "def add_polynomials_df(df_in, features, degree=2):\n",
    "    df = df_in.copy()\n",
    "    for f in features:\n",
    "        if degree >= 2:\n",
    "            df[f\"{f}_sq\"] = df[f] ** 2\n",
    "    return df\n",
    "\n",
    "def add_ratios_df(df_in, features, denom_count=4):\n",
    "    df = df_in.copy()\n",
    "    base = features[0]\n",
    "    for f in features[1:1+denom_count]:\n",
    "        new_name = f\"{base}_ratio_{f}\"\n",
    "        df[new_name] = df[base] / (df[f] + 1e-8)\n",
    "    return df\n",
    "\n",
    "# ---------------------------\n",
    "# APPLY ENGINEERING TO TRAIN DF\n",
    "# ---------------------------\n",
    "df_enh = df.copy()  # df currently includes X + target handled earlier (we will drop target when needed)\n",
    "\n",
    "# add interactions (limit pairs to avoid too many features)\n",
    "df_enh = add_interactions_df(df_enh, top_features, max_pairs=28)\n",
    "df_enh = add_polynomials_df(df_enh, top_features, degree=2)\n",
    "df_enh = add_ratios_df(df_enh, top_features, denom_count=4)\n",
    "\n",
    "# ensure no infinite / NaN values\n",
    "df_enh = df_enh.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# ---------------------------\n",
    "# RECOMPUTE IMPORTANCE ON ENHANCED SET (OPTIONAL QUICK CHECK)\n",
    "# ---------------------------\n",
    "# Re-derive X,y from enhanced df\n",
    "y_enh = df_enh[TARGET].astype(int)\n",
    "X_enh = df_enh.drop(columns=[TARGET])\n",
    "\n",
    "# Fill missing and scale\n",
    "X_enh = X_enh.fillna(X_enh.mean())\n",
    "scaler_enh = StandardScaler()\n",
    "X_enh_scaled = scaler_enh.fit_transform(X_enh)\n",
    "\n",
    "# Quick logistic to get coefficients for pruning guidance\n",
    "logreg_check = LogisticRegression(max_iter=500, class_weight=\"balanced\", random_state=RANDOM_STATE)\n",
    "logreg_check.fit(X_enh_scaled, y_enh)\n",
    "\n",
    "coef_abs = np.abs(logreg_check.coef_[0])\n",
    "coef_series = pd.Series(coef_abs, index=X_enh.columns).sort_values(ascending=False)\n",
    "\n",
    "# ---------------------------\n",
    "# PRUNE LOW-IMPORTANCE FEATURES\n",
    "# ---------------------------\n",
    "# Use aggregated importance from earlier as baseline; for any engineered features not present,\n",
    "# compute use coefficient magnitude as proxy. Build a combined importance Series.\n",
    "agg_importance = imps_df[\"agg_mean\"].copy()\n",
    "\n",
    "# engineered features may not be in agg_importance; map them via coefficient magnitude (normalized)\n",
    "coef_norm = coef_series / (coef_series.max() + 1e-12)\n",
    "\n",
    "combined_imp = agg_importance.reindex(X_enh.columns).fillna(0).copy()\n",
    "# where combined_imp is zero (new features), fill with coef_norm scaled to mean of agg_importance\n",
    "mean_agg = agg_importance.mean() if agg_importance.shape[0] > 0 else 0.0\n",
    "combined_imp = combined_imp + coef_norm * (mean_agg if mean_agg > 0 else 0.01)\n",
    "\n",
    "# normalize to sum to 1 for interpretable thresholding\n",
    "combined_imp_norm = combined_imp / (combined_imp.sum() + 1e-12)\n",
    "\n",
    "# drop features with < threshold importance (relative)\n",
    "PRUNE_THRESHOLD = 0.001  # drop features contributing less than 0.1% of total importance\n",
    "low_features = combined_imp_norm[combined_imp_norm < PRUNE_THRESHOLD].index.tolist()\n",
    "print(f\"\\nPruning {len(low_features)} features with combined importance < {PRUNE_THRESHOLD}\")\n",
    "X_pruned = X_enh.drop(columns=low_features, errors=\"ignore\")\n",
    "\n",
    "print(f\"Features remaining after prune: {X_pruned.shape[1]}\")\n",
    "\n",
    "# ---------------------------\n",
    "# FINAL X,y, SCALE and PIPELINE\n",
    "# ---------------------------\n",
    "X_final = X_pruned.copy()\n",
    "y_final = y_enh.copy()\n",
    "\n",
    "# scale for downstream PCA and models\n",
    "scaler = StandardScaler()\n",
    "X_final_scaled = scaler.fit_transform(X_final)\n",
    "\n",
    "# ---------------------------\n",
    "# TRAINING FUNCTION (uses PCA + SMOTE for MLP)\n",
    "# ---------------------------\n",
    "def train_models(X_scaled_all, y_all, test_size, n_components=None):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_scaled_all, y_all, test_size=test_size, random_state=RANDOM_STATE, stratify=y_all\n",
    "    )\n",
    "\n",
    "    print(f\"\\n======== Test Size = {test_size} ========\")\n",
    "\n",
    "    # PCA\n",
    "    pca = PCA(n_components=n_components or 0.95, random_state=RANDOM_STATE)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_val_pca = pca.transform(X_val)\n",
    "\n",
    "    print(f\"PCA reduced {X_train.shape[1]} → {X_train_pca.shape[1]} features\")\n",
    "\n",
    "    # SMOTE (MLP only)\n",
    "    sm = SMOTE(random_state=RANDOM_STATE)\n",
    "    X_train_bal, y_train_bal = sm.fit_resample(X_train_pca, y_train)\n",
    "\n",
    "    # ---------------- MLP ----------------\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=(32, 16),\n",
    "        activation=\"relu\",\n",
    "        solver=\"adam\",\n",
    "        learning_rate_init=0.001,\n",
    "        alpha=0.0005,\n",
    "        batch_size=64,\n",
    "        early_stopping=True,\n",
    "        max_iter=400,\n",
    "        n_iter_no_change=15,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    mlp.fit(X_train_bal, y_train_bal)\n",
    "    mlp_acc = accuracy_score(y_val, mlp.predict(X_val_pca))\n",
    "\n",
    "    # ---------------- LogReg ----------------\n",
    "    logreg = LogisticRegression(max_iter=500, class_weight=\"balanced\", random_state=RANDOM_STATE)\n",
    "    logreg.fit(X_train_pca, y_train)\n",
    "    logreg_acc = accuracy_score(y_val, logreg.predict(X_val_pca))\n",
    "\n",
    "    # ---------------- Bayes ----------------\n",
    "    bayes = GaussianNB()\n",
    "    bayes.fit(X_train_pca, y_train)\n",
    "    bayes_acc = accuracy_score(y_val, bayes.predict(X_val_pca))\n",
    "\n",
    "    # ---------------- SVM GridSearch ----------------\n",
    "    svm_param_grid = {\n",
    "        \"C\": [0.3, 1, 3, 10],\n",
    "        \"gamma\": [\"scale\", \"auto\", 1e-4, 3e-4, 1e-3],\n",
    "        \"kernel\": [\"rbf\"]\n",
    "    }\n",
    "\n",
    "    svm = SVC(class_weight=\"balanced\", random_state=RANDOM_STATE)\n",
    "\n",
    "    svm_cv = GridSearchCV(\n",
    "        svm,\n",
    "        svm_param_grid,\n",
    "        scoring=\"accuracy\",\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    svm_cv.fit(X_train_pca, y_train)\n",
    "    svm_best = svm_cv.best_estimator_\n",
    "\n",
    "    svm_acc = accuracy_score(y_val, svm_best.predict(X_val_pca))\n",
    "\n",
    "    print(f\"MLP Acc     = {mlp_acc:.4f}\")\n",
    "    print(f\"LogReg Acc  = {logreg_acc:.4f}\")\n",
    "    print(f\"Bayes Acc   = {bayes_acc:.4f}\")\n",
    "    print(f\"SVM Acc     = {svm_acc:.4f}\")\n",
    "    print(f\"Best SVM Params: {svm_cv.best_params_}\")\n",
    "\n",
    "    return mlp, logreg, bayes, svm_best, pca\n",
    "\n",
    "# ---------------------------\n",
    "# TRAIN for two test sizes (like your original)\n",
    "# ---------------------------\n",
    "mlp_02, logreg_02, bayes_02, svm_02, pca_02 = train_models(X_final_scaled, y_final, test_size=0.2)\n",
    "mlp_08, logreg_08, bayes_08, svm_08, pca_08 = train_models(X_final_scaled, y_final, test_size=0.8)\n",
    "\n",
    "# ---------------------------\n",
    "# PREPROCESS TEST SET: apply same feature engineering and pruning\n",
    "# ---------------------------\n",
    "test_df = test.copy()\n",
    "\n",
    "# Apply same get_dummies for categorical columns we used originally.\n",
    "# Note: original cat_cols were derived from train before get_dummies; ensure same columns here.\n",
    "test_df = pd.get_dummies(test_df, columns=cat_cols, drop_first=True)\n",
    "\n",
    "# Reindex to include any missing columns from training (X_final columns + the pruned ones we dropped)\n",
    "# First, create engineered features on test_df using the same functions and top_features\n",
    "# BUT because test_df may be missing some original dummy columns, reindex below to full set with zeros.\n",
    "\n",
    "# Add engineered features to test_df (they will reference columns from test that may not exist yet;\n",
    "# we will create missing columns with zeros after reindexing)\n",
    "test_df = add_interactions_df(test_df, top_features, max_pairs=28)\n",
    "test_df = add_polynomials_df(test_df, top_features, degree=2)\n",
    "test_df = add_ratios_df(test_df, top_features, denom_count=4)\n",
    "\n",
    "# Replace infinities and NaNs\n",
    "test_df = test_df.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# Reindex test_df to have exactly the columns of X_enh (before pruning) but missing columns will be filled with 0\n",
    "test_df = test_df.reindex(columns=X_enh.columns, fill_value=0)\n",
    "\n",
    "# Now apply pruning: remove the features we dropped (low_features)\n",
    "test_df_pruned = test_df.drop(columns=low_features, errors=\"ignore\")\n",
    "\n",
    "# Final feature order should match X_final.columns\n",
    "test_df_pruned = test_df_pruned.reindex(columns=X_final.columns, fill_value=0)\n",
    "\n",
    "# scale using the same scaler used for training (scaler variable)\n",
    "test_scaled = scaler.transform(test_df_pruned)\n",
    "\n",
    "# transform via PCA (two pca models)\n",
    "test_pca_02 = pca_02.transform(test_scaled)\n",
    "test_pca_08 = pca_08.transform(test_scaled)\n",
    "\n",
    "# ---------------------------\n",
    "# SAVE RESULTS (same save function)\n",
    "# ---------------------------\n",
    "def save_output(filename, labels):\n",
    "    pd.DataFrame({\n",
    "        \"patient_id\": test[\"patient_id\"],\n",
    "        \"has_copd_risk\": labels\n",
    "    }).to_csv(filename, index=False)\n",
    "\n",
    "save_output(\"copd_predictions_mlp_02.csv\", mlp_02.predict(test_pca_02))\n",
    "save_output(\"copd_predictions_logreg_02.csv\", logreg_02.predict(test_pca_02))\n",
    "save_output(\"copd_predictions_bayes_02.csv\", bayes_02.predict(test_pca_02))\n",
    "save_output(\"copd_predictions_svm_02.csv\", svm_02.predict(test_pca_02))\n",
    "\n",
    "save_output(\"copd_predictions_mlp_08.csv\", mlp_08.predict(test_pca_08))\n",
    "save_output(\"copd_predictions_logreg_08.csv\", logreg_08.predict(test_pca_08))\n",
    "save_output(\"copd_predictions_bayes_08.csv\", bayes_08.predict(test_pca_08))\n",
    "save_output(\"copd_predictions_svm_08.csv\", svm_08.predict(test_pca_08))\n",
    "\n",
    "print(\"\\nAll predictions saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f64af2f-dcdc-46b0-8814-8324f483ae79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
